!pip install tensorflow

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import re

text_data = ind

def custom_tokenizer(query):
    iso_date_pattern = r"ISODate\('\d{4}-\d{2}-\d{2}'\)"
    date_pattern = r"\d{2}-\d{2}-\d{4}"
    combined_pattern = f"{iso_date_pattern}|{date_pattern}"
    tokens = re.findall(r"\S+|[(){}[\],.$'\"']", query)
    tokenized_query = []
    inside_isodate = False
    for token in tokens:
        if re.match(iso_date_pattern, token) or re.match(date_pattern, token):
            date_str = token[token.find("(") + 2:token.find(")") - 1]
            date_parts = date_str.split('-')
            tokenized_query.extend(date_parts)
        else:
            sub_tokens = token.split('.')
            tokenized_query.extend(sub_tokens)
    return tokenized_query

tokenized_data = [custom_tokenizer(query) for query in text_data]

import numpy as np
from tensorflow.keras.layers import TextVectorization

tokenizer = TextVectorization(output_mode="int")
flat_tokenized_data = np.array([word for query in tokenized_data for word in query])
tokenizer.adapt(flat_tokenized_data)
token_ids = tokenizer(flat_tokenized_data)
token_ids_padded = pad_sequences(token_ids, padding="post")

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = TextVectorization(
    output_mode="int",
    standardize=None,
    split=None,
)
flat_tokenized_data = np.array([word for query in tokenized_data for word in query])
tokenizer.adapt(flat_tokenized_data)
token_ids = tokenizer(flat_tokenized_data)
token_ids_list = [token_ids.numpy().tolist()]
token_ids_padded = pad_sequences(token_ids_list, padding="post")

token = "11"
vocabulary = tokenizer.get_vocabulary()
token_id = vocabulary.index(token)
print(len(vocabulary))
print(token_id)

inp = []
out = []

for index, x in enumerate(tokenized_data):
    if index % 2 == 0:
        inp.append(x)
    else:
        out.append(x)

print(len(out), len(inp))

finalinp = []
finalout = []

for x in inp:
    z = []
    for y in x:
        token = y
        vocabulary = tokenizer.get_vocabulary()
        token_id = vocabulary.index(token)
        z.append(token_id)
    finalinp.append(z)

for x in out:
    z = []
    for y in x:
        token = y
        vocabulary = tokenizer.get_vocabulary()
        token_id = vocabulary.index(token)
        z.append(token_id)
    finalout.append(z)

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenized_lists = finalinp
max_seq_length = 15
padded_sequences = pad_sequences(tokenized_lists, maxlen=max_seq_length, padding='post', truncating='post', value=0)
padded_sequences1 = np.array(padded_sequences)

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

sequences = [[1, 2, 3],
             [4, 5, 6, 7],
             [8, 9, 10, 11, 12]]

padded_sequences = pad_sequences(sequences, padding='post')
shifted_sequences = np.roll(padded_sequences, shift=-1, axis=-1)
onehot_sequences = [to_categorical(seq, num_classes=vocab_size) for seq in shifted_sequences]
target_sequences = np.array(onehot_sequences)

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenized_lists = finalout
max_seq_length = 15
padded_sequences = pad_sequences(tokenized_lists, maxlen=max_seq_length, padding='post', truncating='post', value=0)
padded_sequences2 = np.array(padded_sequences)

print(padded_sequences1[1], padded_sequences2[1])

padded_sequences1.shape
padded_sequences2.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, TimeDistributed
import numpy as np

input_sequences = np.array(padded_sequences1)
target_sequences = np.array(padded_sequences2)

target_sequences_one_hot = tf.keras.utils.to_categorical(target_sequences, num_classes=vocab_size)

embedding_dim = 64
hidden_units = 128
sequence_length = 15
vocab_size = 87

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length))
model.add(LSTM(hidden_units, return_sequences=True))
model.add(TimeDistributed(Dense(vocab_size, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy')

epochs = 100
model.fit(input_sequences, target_sequences_one_hot, epochs=epochs)

test_sequences = np.array([21, 2, 12, 40, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])

predicted_classes = np.argmax(predictions, axis=-1)
print([predicted_classes])

input_sequence_for_inference = input_sequences[-1]
input_sequence_for_inference = tf.expand_dims(input_sequence_for_inference, axis=0)
predicted_sequence = model.predict(input_sequence_for_inference)

print("Predicted Sequence:")
print(predicted_sequence)
